{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((281, 34), (281,), (70, 34), (70,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "# load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"./ion.mat\")\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "xTrIon.shape, yTrIon.shape, xTeIon.shape, yTeIon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"Tree class.\n",
    "    \n",
    "    (You don't need to add any methods or fields here but feel\n",
    "    free to if you like. Our tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly!)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, parent, cutoff_id, cutoff_val, prediction):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.parent = parent\n",
    "        self.cutoff_id = cutoff_id\n",
    "        self.cutoff_val = cutoff_val\n",
    "        self.prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe\n",
    "\n",
    "xTrSpiral,yTrSpiral,xTeSpiral,yTeSpiral=spiraldata(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqsplit(xTr,yTr,weights=None):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights is None: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Optimal the split\n",
    "    c_list = np.zeros(D, 1)\n",
    "    for j in range(D):\n",
    "        X = xTr[:,j]\n",
    "        Y = yTr\n",
    "        sorted_idx = np.argsort(X)\n",
    "        X_sorted, Y_sorted = X[sorted_idx], Y[sorted_idx]\n",
    "        c_best = None\n",
    "\n",
    "        for i in range(len(X_sorted) - 1):\n",
    "            c = (X_sorted[i] + X_sorted[i + 1]) / 2  # Midpoint \n",
    "            left_mask = X_sorted <= c\n",
    "            right_mask = X_sorted > c\n",
    "    \n",
    "        \n",
    "\n",
    "    return feature, cut, bestloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr, weights=None):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and loss value.\n",
    "    \"\"\"\n",
    "    N, D = xTr.shape\n",
    "    if weights is None:\n",
    "        weights = np.ones(N)\n",
    "    weights = weights / np.sum(weights)  # normalize weights\n",
    "    best_feature = None\n",
    "    best_cut = None\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for j in range(D):\n",
    "        # Sort the data according to the j-th feature\n",
    "        sorted_idx = np.argsort(xTr[:, j])\n",
    "        sorted_xTr = xTr[sorted_idx, j]\n",
    "        sorted_yTr = yTr[sorted_idx]\n",
    "        sorted_weights = weights[sorted_idx]\n",
    "\n",
    "        # Compute the total weight and weighted sum of labels\n",
    "        W_total = np.sum(sorted_weights)\n",
    "        P_total = np.sum(sorted_weights * sorted_yTr)\n",
    "\n",
    "        # Initialize left and right sums\n",
    "        W_L, W_R = 0, W_total\n",
    "        P_L, P_R = 0, P_total\n",
    "        Q_L, Q_R = 0, np.sum(sorted_weights * sorted_yTr**2)\n",
    "        \n",
    "        for i in range(N - 1):\n",
    "            # Move data point from right to left\n",
    "            xi, yi, wi = sorted_xTr[i], sorted_yTr[i], sorted_weights[i]\n",
    "            W_L += wi\n",
    "            W_R -= wi\n",
    "            P_L += wi * yi\n",
    "            P_R -= wi * yi\n",
    "            Q_L += wi * yi**2\n",
    "            Q_R -= wi * yi**2\n",
    "            \n",
    "            # Skip if next value is the same\n",
    "            if sorted_xTr[i] == sorted_xTr[i + 1]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate loss for the current split\n",
    "            loss_L = Q_L - (P_L**2) / W_L if W_L > 0 else 0\n",
    "            loss_R = Q_R - (P_R**2) / W_R if W_R > 0 else 0\n",
    "            loss = loss_L + loss_R\n",
    "            \n",
    "            # Update best split if this is the smallest loss so far\n",
    "            if loss < best_loss:\n",
    "                best_feature = j\n",
    "                best_cut = (sorted_xTr[i] + sorted_xTr[i + 1]) / 2.0\n",
    "                best_loss = loss\n",
    "    \n",
    "    return best_feature, best_cut, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the TreeNode class definition provided, let's implement the CART function\n",
    "\n",
    "def cart(xTr, yTr, depth=np.inf, weights=None):\n",
    "    \"\"\"Builds a CART tree.\n",
    "\n",
    "    Args:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "        maxdepth: maximum tree depth\n",
    "        weights: n-dimensional weight vector for data points\n",
    "\n",
    "    Returns:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    # If no weights are passed, assign equal weights to each data point\n",
    "    if weights is None:\n",
    "        weights = np.ones(n) / float(n)\n",
    "    else:\n",
    "        weights = np.array(weights)  # ensure weights is a numpy array\n",
    "        weights = weights / np.sum(weights)  # normalize weights\n",
    "    \n",
    "    # Base case: if max depth is 0 or there's only one unique label, return a leaf node\n",
    "    if depth == 0 or len(np.unique(yTr)) == 1:\n",
    "        return TreeNode(None, None, None, None, None, np.mean(yTr))\n",
    "    \n",
    "    # Use sqsplit to determine the best feature to split on and the best cutoff\n",
    "    feature, cutoff, _ = sqsplit(xTr, yTr, weights)\n",
    "    \n",
    "    # If no valid split was found (feature is infinity), return a leaf node\n",
    "    if np.isinf(feature):\n",
    "        return TreeNode(None, None, None, None, None, np.mean(yTr))\n",
    "    \n",
    "    # Split the data based on the feature and cutoff value found\n",
    "    left_idx = xTr[:, feature] <= cutoff\n",
    "    right_idx = xTr[:, feature] > cutoff\n",
    "    \n",
    "    # If either side is empty, don't split and return a leaf node\n",
    "    if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "        return TreeNode(None, None, None, None, None, np.mean(yTr))\n",
    "    \n",
    "    # Create the left and right subtrees recursively, decreasing the depth by 1\n",
    "    left_subtree = cart(xTr[left_idx], yTr[left_idx], depth-1, weights[left_idx])\n",
    "    right_subtree = cart(xTr[right_idx], yTr[right_idx], depth-1, weights[right_idx])\n",
    "    \n",
    "    # Create and return the current tree node linking to the left and right subtrees\n",
    "    return TreeNode(left_subtree, right_subtree, None, feature, cutoff, None)\n",
    "\n",
    "# To test the cart function, we'll need the sqsplit function to be correctly defined in this environment.\n",
    "# Let's assume that the sqsplit function is available here and is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaltree(root, xTe):\n",
    "    \"\"\"Evaluates xTe using decision tree root.\"\"\"\n",
    "    assert root is not None\n",
    "    \n",
    "    def traverse(node, x):\n",
    "        # Iteratively traverse the tree until a leaf node is reached\n",
    "        while node.cutoff_id is not None:\n",
    "            if x[node.cutoff_id] <= node.cutoff_val:\n",
    "                if node.left is not None:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    return node.prediction\n",
    "            else:\n",
    "                if node.right is not None:\n",
    "                    node = node.right\n",
    "                else:\n",
    "                    return node.prediction\n",
    "        return node.prediction\n",
    "\n",
    "    # Apply traversal for each data point in xTe\n",
    "    n = xTe.shape[0]\n",
    "    pred = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        pred[i] = traverse(root, xTe[i])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Assuming we have a trained tree 'root' and test set 'xTeSpiral':\n",
    "# pred = evaltree(root, xTeSpiral)\n",
    "# Now 'pred' will contain the predictions for 'xTeSpiral' using the provided 'root' tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosttree(x, y, maxiter=100, maxdepth=2):\n",
    "    \"\"\"Learns a boosted decision tree.\n",
    "    Input:\n",
    "        x:        n x d matrix of data points\n",
    "        y:        n-dimensional vector of labels\n",
    "        maxiter:  maximum number of trees\n",
    "        maxdepth: maximum depth of a tree\n",
    "    Output:\n",
    "        forest: list of TreeNode decision trees of length m\n",
    "        alphas: m-dimensional weight vector\n",
    "    \"\"\"\n",
    "    assert np.allclose(np.unique(y), np.array([-1, 1]))  # the labels must be -1 and 1\n",
    "    n, d = x.shape\n",
    "    weights = np.ones(n) / n\n",
    "    preds = np.zeros(n)\n",
    "    forest = []\n",
    "    alphas = []\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        # Train a tree with the current weights\n",
    "        tree = cart(x, y, depth=maxdepth, weights=weights)\n",
    "        # Make predictions with the tree\n",
    "        pred = evaltree(tree, x)\n",
    "        # Compute weighted error rate\n",
    "        weighted_error = np.sum(weights * (pred != y))\n",
    "        # Compute alpha (tree weight), which is based on the error rate\n",
    "        alpha = 0.5 * np.log((1 - weighted_error) / max(weighted_error, 1e-16))\n",
    "        # Update the weights of the data points\n",
    "        weights *= np.exp(-alpha * y * pred)\n",
    "        weights /= np.sum(weights)  # Normalize the weights\n",
    "        \n",
    "        # Store the tree and its corresponding alpha\n",
    "        forest.append(tree)\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "        # Update the predictions with the current tree\n",
    "        preds += alpha * pred\n",
    "        # Check if the predictions are perfect\n",
    "        if np.all(np.sign(preds) == y):\n",
    "            break  # Stop if predictions are perfect\n",
    "    \n",
    "    return forest, alphas\n",
    "\n",
    "# Now let's create the boosted trees using the spiral data\n",
    "forest, alphas = boosttree(xTrSpiral, yTrSpiral, maxiter=100, maxdepth=2)\n",
    "\n",
    "# We will use evalforest to evaluate the boosted trees\n",
    "# We need to redefine evalforest function to include the alphas\n",
    "def evalforest(trees, X, alphas=None):\n",
    "    m = len(trees)\n",
    "    n, d = X.shape\n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / m\n",
    "    pred = np.zeros(n)\n",
    "    for alpha, tree in zip(alphas, trees):\n",
    "        pred += alpha * evaltree(tree, X)\n",
    "    return np.sign(pred)\n",
    "\n",
    "# Evaluate the boosted forest on the training and testing data\n",
    "training_preds = evalforest(forest, xTrSpiral, alphas)\n",
    "testing_preds = evalforest(forest, xTeSpiral, alphas)\n",
    "\n",
    "# Calculate the error rates\n",
    "training_error = np.mean(training_preds != yTrSpiral)\n",
    "testing_error = np.mean(testing_preds != yTeSpiral)\n",
    "\n",
    "print(f\"Boosted forest training error: {training_error:.4f}\")\n",
    "print(f\"Boosted forest testing error: {testing_error:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
